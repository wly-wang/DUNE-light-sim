#!/bin/bash
:<<'EOF'

To use this jobscript to process 5 files from the dataset fardet-hd__fd_mc_2023a_reco2__full-reconstructed__v09_81_00d02__standard_reco2_dune10kt_nu_1x2x6__prodgenie_nu_dune10kt_1x2x6__out1__validation
data and put the output in the $USER namespace (MetaCat) and saves the output in /scratch
Use this command to create the workflow:

justin simple-workflow \
--mql \
"files from dune:all where core.file_type=detector and core.run_type=hd-protodune and core.data_tier=raw limit 10" \
--jobscript submit_local_fcl.jobscript --rss-mb 4000 --max-distance 30 --scope usertests \
--output-pattern "*_reco_*.root:$FNALURL/$USERF" \
--env INPUT_TAR_DIR_LOCAL="$INPUT_TAR_DIR_LOCAL" --env NUM_EVENTS=1

The following optional environment variables can be set when creating the
workflow/stage: FCL_FILE, NUM_EVENTS, DUNE_VERSION, DUNE_QUALIFIER 

EOF

# fcl file and DUNE software version/qualifier to be used
FCL_FILE=${FCL_FILE:-$INPUT_TAR_DIR_LOCAL/to_grid/dune10kt_v6_refactored_1x2x6_light_sim.fcl}
DUNE_VERSION=${DUNE_VERSION:-v10_03_01d02}
DUNE_QUALIFIER=${DUNE_QUALIFIER:-e26:prof}

# number of events to process from the input file
if [ "$NUM_EVENTS" != "" ] ; then
 events_option="-n $NUM_EVENTS"
fi

# # First get an unprocessed file from this stage
# did_pfn_rse=`$JUSTIN_PATH/justin-get-file`

# if [ "$did_pfn_rse" = "" ] ; then
#   echo "Nothing to process - exit jobscript"
#   exit 0
# fi

# # Keep a record of all input DIDs, for pdjson2meta file -> DID mapping
# echo "$did_pfn_rse" | cut -f1 -d' ' >>all-input-dids.txt

# # pfn is also needed when creating justin-processed-pfns.txt
# pfn=`echo $did_pfn_rse | cut -f2 -d' '`
# echo "Input PFN = $pfn"

# Setup DUNE environment
source /cvmfs/dune.opensciencegrid.org/products/dune/setup_dune.sh
setup dunesw "$DUNE_VERSION" -q "$DUNE_QUALIFIER"
source $INPUT_TAR_DIR_LOCAL/dune_light_sim_v10_03_01d02/localProducts_larsoft_v10_03_01d02_e26_prof/setup
mrbslp

# Construct outFile from input $pfn 
now=$(date -u +"%Y-%m-%dT_%H%M%SZ")
# Ffname=`echo $pfn | awk -F/ '{print $NF}'`
# fname=`echo $Ffname | awk -F. '{print $1}'`
outFile=${now}_reco_${now}.root

campaign="justIN.w${JUSTIN_WORKFLOW_ID}s${JUSTIN_STAGE_ID}"

(
# Do the scary preload stuff in a subshell!
export LD_PRELOAD=${XROOTD_LIB}/libXrdPosixPreload.so
echo "$LD_PRELOAD"

lar -c $FCL_FILE $events_option -o $outFile > ${now}_reco_${now}.log 2>&1
)

echo '=== Start last 100 lines of lar log file ==='
tail -100 ${now}_reco_${now}.log
echo '=== End last 100 lines of lar log file ==='

# Subshell exits with exit code of last command
larExit=$?
echo "lar exit code $larExit"
echo $now

if [ $larExit -eq 0 ] ; then
  # Success !
  echo This job is done with output file $outFile created at $now
  jobscriptExit=0
else
  # Oh :(
  jobscriptExit=1
fi

# Create compressed tar file with all log files 
tar zcf `echo "$JUSTIN_JOBSUB_ID.logs.tgz" | sed 's/@/_/g'` *.log
exit $jobscriptExit
